================================================================================
              FLIGHT PRICE ANALYSIS PIPELINE - CODE REVIEW GUIDE
================================================================================

This guide explains every code component in beginner-friendly terms. It's designed
for someone new to data engineering, Airflow, dbt, and Docker.

================================================================================
                    HOW TO RUN THIS PROJECT (Visual Studio Code)
================================================================================

PREREQUISITES:
Before you start, make sure you have these installed on your computer:

1. Docker Desktop
   - Download from: https://www.docker.com/products/docker-desktop
   - After installing, open Docker Desktop and wait until it says "Docker is running"
   - You'll see a green icon in your system tray when it's ready

2. Visual Studio Code
   - Download from: https://code.visualstudio.com/
   - Install the "Docker" extension (optional but helpful)

--------------------------------------------------------------------------------
STEP-BY-STEP: RUNNING THE PROJECT IN VS CODE
--------------------------------------------------------------------------------

STEP 1: Open the Project Folder
   - Open VS Code
   - Click File > Open Folder
   - Navigate to: c:\Users\OliviaDosimey\Desktop\Airflow_Flight_Price_Analysis
   - Click "Select Folder"

STEP 2: Open the Terminal
   - In VS Code, click Terminal > New Terminal (or press Ctrl + `)
   - A terminal panel will appear at the bottom of VS Code
   - Make sure it shows the project folder path

STEP 3: Start Docker Desktop
   - Before running commands, make sure Docker Desktop is running
   - Look for the Docker whale icon in your system tray (bottom right)
   - If it's not running, open Docker Desktop and wait for it to start

STEP 4: Build and Start All Containers
   - In the VS Code terminal, type this command and press Enter:
   
     docker-compose up -d --build
   
   WHAT THIS DOES:
   - "docker-compose" = Tool that manages multiple containers
   - "up" = Start the containers
   - "-d" = Run in background (detached mode) so you can use the terminal
   - "--build" = Rebuild the images with any changes you made
   
   EXPECTED OUTPUT:
   - You'll see containers being created for:
     * flight_airflow_postgres (Airflow's database)
     * flight_mysql (Staging database)
     * flight_postgres_analytics (Analytics database)
     * flight_airflow_webserver (Airflow UI)
     * flight_airflow_scheduler (Runs the tasks)
   
   WAIT TIME: This takes about 1-2 minutes the first time.

STEP 5: Check That Everything is Running
   - Run this command:
   
     docker-compose ps
   
   WHAT TO LOOK FOR:
   - All containers should show "healthy" in the STATUS column
   - If you see "starting", wait 30 seconds and run the command again

STEP 6: Trigger the Pipeline
   - Run this command to start the data pipeline:
   
     docker exec flight_airflow_webserver airflow dags trigger flight_price_pipeline
   
   WHAT THIS DOES:
   - "docker exec" = Run a command inside a container
   - "flight_airflow_webserver" = The container we're running in
   - "airflow dags trigger" = Tell Airflow to start a DAG run
   - "flight_price_pipeline" = The name of our pipeline

STEP 7: Check the Pipeline Progress
   - Run this command to see task status:
   
     docker exec flight_airflow_webserver airflow dags list-runs -d flight_price_pipeline
   
   - Wait about 60 seconds, then check task details:
   
     docker exec flight_airflow_webserver airflow tasks states-for-dag-run flight_price_pipeline "<run_id>"
   
   (Replace <run_id> with the run ID shown in the previous command, like "manual__2026-02-02T09:13:53+00:00")

STEP 8: Access the Airflow Web Interface (Optional)
   - Open your web browser
   - Go to: http://localhost:8080
   - Login with:
     * Username: airflow
     * Password: airflow
   - Click on "flight_price_pipeline" to see the DAG graph

STEP 9: Stop the Containers (When Done)
   - When you're finished, stop everything with:
   
     docker-compose down
   
   - This stops and removes all containers (data is preserved in volumes)

--------------------------------------------------------------------------------
TROUBLESHOOTING COMMON ISSUES
--------------------------------------------------------------------------------

ISSUE: "Cannot connect to Docker daemon"
SOLUTION: Make sure Docker Desktop is running. Open it and wait for the green status.

ISSUE: Container shows "unhealthy"
SOLUTION: Run "docker-compose down" then "docker-compose up -d --build" again.

ISSUE: "port is already in use"
SOLUTION: Another application is using port 8080, 3307, or 5433. Close that app or 
          change the ports in docker-compose.yml.

ISSUE: DAG shows "No module named 'validate_and_load_csv'"
SOLUTION: The container needs the scripts mounted. Run "docker-compose down" and 
          "docker-compose up -d --build".

================================================================================
                          TABLE OF CONTENTS
================================================================================

1. PROJECT OVERVIEW (What does this project do?)
2. FILE STRUCTURE (Where is everything?)
3. DOCKER FILES (How do containers work?)
4. SQL SCHEMAS (How is data stored?)
5. PYTHON SCRIPTS (What code runs?)
6. AIRFLOW DAG (How is everything orchestrated?)
7. DBT MODELS (How is data transformed?)
8. DATA FLOW (How does data move through the system?)

================================================================================
1. PROJECT OVERVIEW
================================================================================

WHAT THIS PROJECT DOES:
-----------------------
This project analyzes flight prices in Bangladesh. It takes a CSV file with
flight booking data and turns it into useful business insights (KPIs).

THINK OF IT LIKE:
-----------------
Imagine you have a giant spreadsheet with flight data. This pipeline:
1. Checks if the spreadsheet is valid (no errors)
2. Cleans up messy data (spelling mistakes, missing values)
3. Organizes it into neat tables (like separating airline info from price info)
4. Calculates useful numbers (average prices, most popular routes)

THE MEDALLION ARCHITECTURE:
---------------------------
We use a "Bronze â†’ Silver â†’ Gold" pattern (like medals!):

  BRONZE = Raw Data (just cleaned enough to store)
           Think: "Here's everything from the spreadsheet, organized"
           
  SILVER = Cleaned & Structured (organized into proper tables)
           Think: "Here's the data split into Airline table, Route table, etc."
           
  GOLD = Business Insights (calculations and summaries)
         Think: "Here are the answers to business questions"

THE 5 PIPELINE TASKS:
---------------------
When you run the pipeline, these 5 things happen in order:

  [1] validate_and_load_csv   â†’ Check CSV is valid, load into MySQL
  [2] transfer_to_bronze      â†’ Move from MySQL to PostgreSQL Bronze layer
  [3] dbt_silver              â†’ Create dimension tables and fact table
  [4] dbt_gold                â†’ Calculate KPIs (average fares, popular routes)
  [5] generate_report         â†’ Print a summary of what was done

Total time: About 50 seconds.

================================================================================
2. FILE STRUCTURE
================================================================================

Here's what each file/folder does:

Airflow_Flight_Price_Analysis/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ flight_price_pipeline.py    â† THE MAIN FILE! Controls everything
â”‚
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ sql/
â”‚   â”‚   â”œâ”€â”€ mysql_staging_schema.sql    â† Creates MySQL tables
â”‚   â”‚   â””â”€â”€ bronze_schema.sql           â† Creates PostgreSQL tables
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ validate_and_load_csv.py    â† Task 1: Validates & loads CSV
â”‚       â””â”€â”€ transfer_to_bronze.py       â† Task 2: Moves data to PostgreSQL
â”‚
â”œâ”€â”€ dbt/
â”‚   â””â”€â”€ flight_analytics/
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ silver/                 â† Task 3: Creates clean tables
â”‚       â”‚   â”‚   â”œâ”€â”€ stg_flight_bookings.sql
â”‚       â”‚   â”‚   â”œâ”€â”€ dim_airline.sql
â”‚       â”‚   â”‚   â”œâ”€â”€ dim_route.sql
â”‚       â”‚   â”‚   â”œâ”€â”€ dim_date.sql
â”‚       â”‚   â”‚   â””â”€â”€ fct_flight_bookings.sql
â”‚       â”‚   â””â”€â”€ gold/                   â† Task 4: Creates KPI tables
â”‚       â”‚       â”œâ”€â”€ kpi_avg_fare_by_airline.sql
â”‚       â”‚       â”œâ”€â”€ kpi_booking_count_by_airline.sql
â”‚       â”‚       â”œâ”€â”€ kpi_most_popular_routes.sql
â”‚       â”‚       â””â”€â”€ kpi_seasonal_fare_variation.sql
â”‚       â”œâ”€â”€ packages.yml                â† Lists dbt dependencies
â”‚       â””â”€â”€ profiles.yml                â† Database connection settings
â”‚
â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ Flight_Price_Dataset_of_Bangladesh.csv   â† THE SOURCE DATA
â”‚
â”œâ”€â”€ Dockerfile                          â† How to build the Airflow container
â”œâ”€â”€ docker-compose.yml                  â† Defines all services
â”œâ”€â”€ requirements.txt                    â† Python packages to install
â”œâ”€â”€ .env                                â† Environment variables (passwords etc.)
â”œâ”€â”€ .gitignore                          â† Files Git should ignore
â””â”€â”€ README.md                           â† Project documentation

================================================================================
3. DOCKER CONFIGURATION
================================================================================

WHAT IS DOCKER?
---------------
Docker creates isolated "containers" - think of them as mini-computers running
inside your computer. Each container has one job:

  Container 1: flight_mysql           â†’ Stores raw data (staging)
  Container 2: flight_postgres_analytics â†’ Stores transformed data (Bronze/Silver/Gold)
  Container 3: flight_airflow_postgres   â†’ Stores Airflow's own data
  Container 4: flight_airflow_webserver  â†’ Runs the Airflow website UI
  Container 5: flight_airflow_scheduler  â†’ Actually runs the pipeline tasks

--------------------------------------------------------------------------------
3.1 Dockerfile
--------------------------------------------------------------------------------

LOCATION: Dockerfile (in project root)

WHAT IT DOES:
This file is a recipe for building the Airflow container.

LINE-BY-LINE EXPLANATION:

  Line 2: FROM apache/airflow:2.10.4-python3.11
  
    "Start with the official Airflow image that already has Airflow installed"
    (Like starting with a pre-made cake mix instead of flour and eggs)

  Lines 4-10: System dependencies
  
    USER root                 â† Switch to admin user (needed for installing)
    RUN apt-get update...     â† Install tools needed for database connections
    USER airflow              â† Switch back to normal user (safer)

  Lines 15-16: Python packages
  
    COPY requirements.txt ... â† Copy our list of needed Python packages
    RUN pip install ...       â† Install them (pandas, mysql-connector, etc.)

  Lines 19-22: dbt project
  
    COPY dbt/... ...          â† Copy our dbt models into the container
    ENV DBT_PROFILES_DIR=...  â† Tell dbt where to find its config file

--------------------------------------------------------------------------------
3.2 docker-compose.yml  
--------------------------------------------------------------------------------

LOCATION: docker-compose.yml (in project root)

WHAT IT DOES:
Defines ALL containers and how they connect to each other.

SERVICES EXPLAINED:

1. postgres (Airflow's internal database)
   - This is NOT where your data goes
   - Airflow uses this to remember what tasks ran, when, etc.
   - Port 5432 (internal only - you don't need to access this)

2. mysql (Staging database - YOUR DATA FIRST GOES HERE)
   - Port 3307 on your computer â†’ Port 3306 inside container
   - Username: airflow, Password: airflow
   - Database name: flight_staging
   - Table: raw_flight_staging (stores the raw CSV data)

3. postgres_analytics (Analytics database - YOUR TRANSFORMED DATA)
   - Port 5433 on your computer â†’ Port 5432 inside container
   - Username: analytics, Password: analytics
   - Database name: flight_analytics
   - Has 3 schemas: bronze, bronze_silver, bronze_gold

4. airflow-webserver (The Airflow UI)
   - Port 8080 on your computer
   - Go to http://localhost:8080 to see it
   - Login: airflow / airflow

5. airflow-scheduler (Runs your tasks)
   - No port needed - works in the background
   - Watches for DAGs to trigger and runs them

6. airflow-init (One-time setup)
   - Only runs once when you first start
   - Creates the airflow user and sets up the database

VOLUMES (Shared folders):
   "./dags:/opt/airflow/dags"
   
   This means: The dags folder on YOUR computer is visible inside the container.
   When you edit flight_price_pipeline.py, the container sees the changes.

================================================================================
4. SQL SCHEMA FILES
================================================================================

These files create the database tables when containers first start.

--------------------------------------------------------------------------------
4.1 mysql_staging_schema.sql
--------------------------------------------------------------------------------

LOCATION: include/sql/mysql_staging_schema.sql

WHAT IT DOES:
Creates the MySQL tables that store raw CSV data.

TABLES CREATED:

Table 1: raw_flight_staging
   - id (auto-incrementing number)
   - Airline, Source, Destination, etc. (columns matching the CSV)
   - ingested_at (timestamp when row was added)
   
   WHY: This is exactly like the CSV, just in database form.

Table 2: ingestion_log
   - Tracks whether CSV loading succeeded or failed
   - Stores: file name, status (SUCCESS/FAILURE), row counts, errors
   
   WHY: For debugging - if something goes wrong, you can see what happened.

--------------------------------------------------------------------------------
4.2 bronze_schema.sql
--------------------------------------------------------------------------------

LOCATION: include/sql/bronze_schema.sql

WHAT IT DOES:
Creates the PostgreSQL schemas and Bronze layer table.

SCHEMAS CREATED:
   - bronze: Raw data from MySQL with cleaned column names
   - silver: Will hold dimension and fact tables  
   - gold: Will hold KPI tables

TABLE: bronze.raw_flight_data
   - Same columns as MySQL, but with "snake_case" names
   - Example: "Base Fare (BDT)" becomes "base_fare_bdt"
   - Also adds: source_system (where data came from), ingested_at

TABLE: bronze.transfer_log
   - Tracks data transfers from MySQL to PostgreSQL
   - For debugging and auditing

================================================================================
5. PYTHON SCRIPTS
================================================================================

These are the Python files that do the actual work.

--------------------------------------------------------------------------------
5.1 validate_and_load_csv.py
--------------------------------------------------------------------------------

LOCATION: include/scripts/validate_and_load_csv.py

WHAT IT DOES (Task 1):
1. Opens the CSV file
2. Checks it's valid (has all columns, correct data types)
3. Loads it into MySQL

STEP-BY-STEP WHAT HAPPENS:

  Step 1: VALIDATION
    - Does the file exist?
    - Is it empty?
    - Does it have all 17 required columns? (Airline, Source, etc.)
    - Are the data types correct? (numbers are numbers, text is text)
    
    If ANY check fails â†’ Log the error and stop

  Step 2: LOADING
    - Connect to MySQL database
    - Delete any existing data (clean start each time)
    - Insert rows in batches of 5000 (faster than one at a time)
    - Verify row count matches CSV

  Step 3: LOGGING
    - Record success/failure in the ingestion_log table

KEY FUNCTION:
  run_validation_and_load(**context)
    - This is what Airflow calls
    - Gets database connection details from Airflow
    - Calls the validation and loading functions

--------------------------------------------------------------------------------
5.2 transfer_to_bronze.py
--------------------------------------------------------------------------------

LOCATION: include/scripts/transfer_to_bronze.py

WHAT IT DOES (Task 2):
Moves data from MySQL to PostgreSQL, renaming columns along the way.

STEP-BY-STEP WHAT HAPPENS:

  Step 1: EXTRACT
    - Connect to MySQL
    - Run: SELECT * FROM raw_flight_staging
    - Put results in a pandas DataFrame

  Step 2: TRANSFORM
    - Rename columns from "Title Case" to "snake_case"
    - Examples:
      "Airline" â†’ "airline"
      "Source Name" â†’ "source_name"  
      "Base Fare (BDT)" â†’ "base_fare_bdt"
      "Class" â†’ "booking_class" (avoids SQL keyword conflict)
    - Remove MySQL-only columns (id, ingested_at)

  Step 3: LOAD
    - Connect to PostgreSQL
    - Delete existing Bronze data (clean start)
    - Insert rows using execute_values() (very fast method)
    - Verify row count matches

COLUMN_MAPPING dictionary:
  This defines exactly how each column gets renamed.
  It's like a translation table.

================================================================================
6. AIRFLOW DAG
================================================================================

LOCATION: dags/flight_price_pipeline.py

WHAT IS A DAG?
--------------
DAG = Directed Acyclic Graph (fancy term for "a series of tasks in order")

Think of it like a recipe:
  Step 1: Do this â†’ Step 2: Then this â†’ Step 3: Then this â†’ Done!

Each step depends on the previous one finishing successfully.

--------------------------------------------------------------------------------
THE 5 TASKS IN THIS DAG:
--------------------------------------------------------------------------------

Task 1: validate_and_load_csv
   Type: PythonOperator
   What: Runs the validate_and_load_csv.py script
   Result: CSV data is now in MySQL

Task 2: transfer_to_bronze
   Type: PythonOperator
   What: Runs the transfer_to_bronze.py script
   Result: Data is now in PostgreSQL Bronze layer

Task 3: dbt_silver
   Type: BashOperator
   What: Runs these dbt commands:
         - dbt deps (install packages)
         - dbt run --select silver (create Silver tables)
   Result: Dimension tables and fact table created

Task 4: dbt_gold
   Type: BashOperator
   What: Runs: dbt run --select gold
   Result: KPI tables created

Task 5: generate_report
   Type: PythonOperator
   What: Queries all tables and prints statistics
   Result: Summary report in the logs

TASK DEPENDENCIES:
   validate_and_load_csv 
       â†’ transfer_to_bronze 
           â†’ dbt_silver 
               â†’ dbt_gold 
                   â†’ generate_report

Each arrow means "wait for this to finish, then proceed".

--------------------------------------------------------------------------------
THE generate_summary_report FUNCTION:
--------------------------------------------------------------------------------

This function runs at the end and prints a report like:

  ============================================================
  FLIGHT PRICE ANALYSIS PIPELINE - EXECUTION REPORT
  ============================================================
  
  ğŸ“¦ BRONZE LAYER:
     Total records: 57,000
  
  ğŸ¥ˆ SILVER LAYER:
     Validated records: 57,000
     Validation rate: 100.0%
     Unique airlines: 24
     Unique routes: 2256
  
  ğŸ¥‡ GOLD LAYER (KPIs):
     Top 3 Airlines by Bookings:
       1. US-Bangla Airlines: 4,496 (7.89%)
       2. Vistara: 2,368 (4.15%)
       3. Lufthansa: 2,368 (4.15%)
     
     Top 3 Popular Routes:
       1. RJH-SIN: 417 bookings (Avg: à§³113,963)
       2. DAC-DXB: 413 bookings
       3. BZL-YYZ: 410 bookings

================================================================================
7. DBT MODELS
================================================================================

WHAT IS DBT?
------------
dbt = "data build tool" - it writes SQL for you and manages your data models.

Instead of writing complicated SQL, you write simple SQL "models" and dbt:
- Runs them in the right order
- Creates tables automatically
- Tests your data quality

--------------------------------------------------------------------------------
7.1 SILVER LAYER MODELS (Creating Clean Data)
--------------------------------------------------------------------------------

These models turn raw Bronze data into organized, clean tables.

--- stg_flight_bookings.sql ---

WHAT IT DOES: Cleans the raw data.

Cleaning steps:
  1. Handle missing values (replace NULL with defaults like 'Unknown')
  2. Trim whitespace from text fields
  3. Fix total fare if it's incorrectly calculated
  4. Remove duplicate rows (keeps first occurrence)
  5. Filter out invalid data (negative fares, impossible durations)

Materialized as: VIEW (not a physical table - runs query each time)

--- dim_airline.sql ---

WHAT IT DOES: Creates a list of unique airlines.

Output:
  - airline_key (unique ID for each airline)
  - airline_name (e.g., "US-Bangla Airlines")

WHY: Instead of storing "US-Bangla Airlines" 4,496 times in the fact table,
we store it once here and just reference the key.

--- dim_route.sql ---

WHAT IT DOES: Creates a list of unique flight routes.

Output:
  - route_key (unique ID)
  - source_code (e.g., "DAC")
  - destination_code (e.g., "DXB")
  - route_code (e.g., "DAC-DXB")
  - route_description (e.g., "Dhaka to Dubai")

--- dim_date.sql ---

WHAT IT DOES: Extracts date info from departure dates.

Output:
  - date_key (e.g., 20250115 for Jan 15, 2025)
  - year, month, day, quarter
  - day_of_week (0=Sunday, 6=Saturday)
  - is_weekend (true/false)
  - weather_season (Winter, Summer, Monsoon)

--- fct_flight_bookings.sql ---

WHAT IT DOES: Creates the main fact table joining everything.

This is the "center" of a star schema:
  - Links to dim_airline via airline_key
  - Links to dim_route via route_key  
  - Links to dim_date via date_key
  - Contains all the numbers (measures):
    * base_fare_bdt
    * tax_surcharge_bdt
    * total_fare_bdt
    * duration_hours
    * days_before_departure

Also calculates:
  - tax_percentage = tax / base_fare * 100
  - booking_lead_category = "Last Minute" / "Standard" / "Early Bird"

--------------------------------------------------------------------------------
7.2 GOLD LAYER MODELS (Business KPIs)
--------------------------------------------------------------------------------

These models answer business questions.

--- kpi_avg_fare_by_airline.sql ---

BUSINESS QUESTION: "What's the average fare for each airline?"

Output includes:
  - Average base fare, tax, total fare per airline
  - Min/max fares
  - Standard deviation (how much prices vary)
  - Market share percentage
  - Rankings

--- kpi_booking_count_by_airline.sql ---

BUSINESS QUESTION: "Which airlines have the most bookings?"

Output includes:
  - Total bookings per airline
  - Breakdown by class (Economy vs Business vs First)
  - Breakdown by booking source (Online vs Agency vs Direct)
  - Peak season bookings
  - Revenue totals

--- kpi_most_popular_routes.sql ---

BUSINESS QUESTION: "What are the most popular flight routes?"

Output includes:
  - Top 50 routes by booking count
  - Average fare and duration per route
  - Direct vs connecting flight breakdown
  - Revenue per route

--- kpi_seasonal_fare_variation.sql ---

BUSINESS QUESTION: "How do prices change during peak seasons?"

Peak seasons defined:
  - Eid ul-Fitr
  - Eid ul-Adha
  - Winter Holidays

Output includes:
  - Average fare during Peak vs Non-Peak seasons
  - Percentage difference (how much more expensive is peak season?)
  - Breakdown by individual season type

================================================================================
8. DATA FLOW SUMMARY
================================================================================

Here's how data moves through the entire system:

STEP 1: CSV FILE
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Flight_Price_Dataset_of_Bangladesh.csv             â”‚
   â”‚  57,000 rows, 17 columns                            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    Task 1: validate_and_load_csv
                    (Checks structure, loads in batches)
                              â”‚
                              â–¼
STEP 2: MYSQL (Staging)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  flight_staging.raw_flight_staging                  â”‚
   â”‚  Original column names (e.g., "Base Fare (BDT)")    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    Task 2: transfer_to_bronze
                    (Renames columns to snake_case)
                              â”‚
                              â–¼
STEP 3: POSTGRESQL BRONZE
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  bronze.raw_flight_data                             â”‚
   â”‚  Snake_case columns (e.g., "base_fare_bdt")         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    Task 3: dbt_silver
                    (Cleans data, creates star schema)
                              â”‚
                              â–¼
STEP 4: POSTGRESQL SILVER
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  bronze_silver.stg_flight_bookings (cleaned data)   â”‚
   â”‚  bronze_silver.dim_airline (24 unique airlines)     â”‚
   â”‚  bronze_silver.dim_route (2,256 unique routes)      â”‚
   â”‚  bronze_silver.dim_date (date attributes)           â”‚
   â”‚  bronze_silver.fct_flight_bookings (main fact)      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    Task 4: dbt_gold
                    (Calculates business KPIs)
                              â”‚
                              â–¼
STEP 5: POSTGRESQL GOLD
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  bronze_gold.kpi_avg_fare_by_airline                â”‚
   â”‚  bronze_gold.kpi_booking_count_by_airline           â”‚
   â”‚  bronze_gold.kpi_most_popular_routes                â”‚
   â”‚  bronze_gold.kpi_seasonal_fare_variation            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    Task 5: generate_report
                    (Queries and prints summary)
                              â”‚
                              â–¼
STEP 6: EXECUTION REPORT
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Pipeline completed successfully! âœ…                â”‚
   â”‚  - 57,000 records processed                         â”‚
   â”‚  - Top airlines, routes, seasonal insights          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

================================================================================
                              GLOSSARY
================================================================================

TERM               WHAT IT MEANS
----               -------------
DAG                Directed Acyclic Graph - a series of tasks in order
dbt                Data Build Tool - runs SQL transformations
Container          Mini-computer running inside Docker
Schema             A folder inside a database (like bronze, silver, gold)
ETL                Extract, Transform, Load - moving and changing data
Star Schema        Data model with fact table in center, dimensions around
Fact Table         Table with measurements (numbers to analyze)
Dimension Table    Table with descriptive info (airline names, dates)
KPI                Key Performance Indicator - important business metric
Medallion          Bronze â†’ Silver â†’ Gold architecture pattern
snake_case         Naming style: words_separated_by_underscores

================================================================================
                              END OF GUIDE
================================================================================

Generated: 2026-02-02
Project: Flight Price Analysis Pipeline  
Architecture: Medallion (Bronze â†’ Silver â†’ Gold)
Technologies: Airflow 2.10.4, MySQL 8.0, PostgreSQL 15, dbt 1.8.0, Docker
